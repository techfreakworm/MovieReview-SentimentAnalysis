{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-team-imdb-lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techfreakworm/MovieReview-SentimentAnalysis/blob/master/keras_team_imdb_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4Z1gXmlDgWZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('kerasLSTM.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IJcPYEdoRTFu",
        "colab_type": "code",
        "outputId": "0a8e6f3e-e170-4d3b-f63d-d877c9566487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as  np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "GQKr8DWjSIql",
        "colab_type": "code",
        "outputId": "2dcd6f5c-f9a4-488c-dfee-19b8c1633e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# set parameters:\n",
        "max_features = 20000\n",
        "maxlen = 100\n",
        "batch_size = 32\n",
        "# embedding_dims = 128\n",
        "# filters = 250\n",
        "# kernel_size = 3\n",
        "# hidden_dims = 250\n",
        "# epochs = 5\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "model.add(Embedding(max_features,\n",
        "                    128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 100)\n",
            "x_test shape: (25000, 100)\n",
            "Build model...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "25000/25000 [==============================] - 262s 10ms/step - loss: 0.4496 - acc: 0.7918 - val_loss: 0.3850 - val_acc: 0.8390\n",
            "Epoch 2/15\n",
            "25000/25000 [==============================] - 261s 10ms/step - loss: 0.2996 - acc: 0.8794 - val_loss: 0.3711 - val_acc: 0.8391\n",
            "Epoch 3/15\n",
            "25000/25000 [==============================] - 261s 10ms/step - loss: 0.2258 - acc: 0.9125 - val_loss: 0.3794 - val_acc: 0.8421\n",
            "Epoch 4/15\n",
            "25000/25000 [==============================] - 262s 10ms/step - loss: 0.1592 - acc: 0.9417 - val_loss: 0.4144 - val_acc: 0.8388\n",
            "Epoch 5/15\n",
            "25000/25000 [==============================] - 262s 10ms/step - loss: 0.1074 - acc: 0.9616 - val_loss: 0.5945 - val_acc: 0.8184\n",
            "Epoch 6/15\n",
            "25000/25000 [==============================] - 262s 10ms/step - loss: 0.0855 - acc: 0.9706 - val_loss: 0.6067 - val_acc: 0.8337\n",
            "Epoch 7/15\n",
            "25000/25000 [==============================] - 262s 10ms/step - loss: 0.0574 - acc: 0.9811 - val_loss: 0.6502 - val_acc: 0.8334\n",
            "Epoch 8/15\n",
            "25000/25000 [==============================] - 262s 10ms/step - loss: 0.0576 - acc: 0.9799 - val_loss: 0.7270 - val_acc: 0.8266\n",
            "Epoch 9/15\n",
            "25000/25000 [==============================] - 264s 11ms/step - loss: 0.0394 - acc: 0.9876 - val_loss: 0.7730 - val_acc: 0.8266\n",
            "Epoch 10/15\n",
            "25000/25000 [==============================] - 263s 11ms/step - loss: 0.0277 - acc: 0.9914 - val_loss: 0.8645 - val_acc: 0.8245\n",
            "Epoch 11/15\n",
            "25000/25000 [==============================] - 264s 11ms/step - loss: 0.0209 - acc: 0.9936 - val_loss: 0.8160 - val_acc: 0.8236\n",
            "Epoch 12/15\n",
            "25000/25000 [==============================] - 265s 11ms/step - loss: 0.0186 - acc: 0.9940 - val_loss: 0.9216 - val_acc: 0.8218\n",
            "Epoch 13/15\n",
            "25000/25000 [==============================] - 266s 11ms/step - loss: 0.0213 - acc: 0.9938 - val_loss: 1.0013 - val_acc: 0.8173\n",
            "Epoch 14/15\n",
            "25000/25000 [==============================] - 265s 11ms/step - loss: 0.0122 - acc: 0.9968 - val_loss: 1.0577 - val_acc: 0.8216\n",
            "Epoch 15/15\n",
            "25000/25000 [==============================] - 265s 11ms/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.9913 - val_acc: 0.8197\n",
            "25000/25000 [==============================] - 56s 2ms/step\n",
            "Test score: 0.991288885307312\n",
            "Test accuracy: 0.81968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lgq7YeugTz6o",
        "colab_type": "code",
        "outputId": "d7402d5a-60b8-4fe3-d884-8c579e15aa7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kqxSG07aexxu",
        "colab_type": "code",
        "outputId": "eacb50db-f506-4e56-c94d-ee88803b1b2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "word_index=imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6U6N_d-7e6X_",
        "colab_type": "code",
        "outputId": "f12fe548-1fb0-4830-f5ad-105dfd15da42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# sentence =  \"I liked the movie very much. My friends said it was good, and they couldn't be any more correct. This is value for both time and money\"\n",
        "# sentence = \"I hated the movie. My Friends said it was shit and they were right. Cast was pathetic as well\"\n",
        "sentence = \"shitty movie\"\n",
        "words = sentence.split()\n",
        "words = [word.lower() for word in words]\n",
        "import string\n",
        "import numpy as np\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "words = [w.translate(table) for w in words]\n",
        "input_array=np.array([[word_index[word]+3 if word in word_index else 0 for word in words]])\n",
        "\n",
        "# print(input_array)\n",
        "input_array = sequence.pad_sequences(input_array, maxlen)\n",
        "# print(input_array)\n",
        "input_array.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "8klA8XT0gj-t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('model_lstm.h5')\n",
        "from google.colab import files\n",
        "files.download('model_lstm.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UE1o3gO2fD35",
        "colab_type": "code",
        "outputId": "b2ef4722-190a-4995-e505-a046fef1420c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "prediction = model.predict_classes(input_array)\n",
        "print(model.predict(input_array))\n",
        "# print(model.predict(input_array))\n",
        "# print(model.predict_classes(input_array))\n",
        "if prediction[0][0] == 0:\n",
        "  print('Bad')\n",
        "if prediction[0][0] == 1:\n",
        "  print('Good')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00215687]]\n",
            "Bad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dHOJutLHfTCc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}